{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-iD-xN6fFxL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdkP3fUFU9L-",
        "outputId": "37ae22e7-013c-4481-9d7e-e2b70fde2735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xatQfEk5h262",
        "outputId": "ba251a8a-82d3-4075-e5a7-b01a11115e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = \"merged_comments.csv\"\n",
        "data = pd.read_csv(csv_file, encoding='utf-8')"
      ],
      "metadata": {
        "id": "I5Set915h_41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tools for pre-processing\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "jJ_hmB08iE5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "english_stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbR7UKyWifxM",
        "outputId": "05396e56-9674-4110-a5a4-d85533f637f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_comments = []"
      ],
      "metadata": {
        "id": "Z8qz9cExisvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-process each comment\n",
        "for comment in data['Text']:\n",
        "\n",
        "    # Remove URLs\n",
        "    comment = re.sub(r'http\\S+', '', comment)\n",
        "\n",
        "    # Remove user names (assuming they start with u/)\n",
        "    comment = re.sub(r'u/\\w+', '', comment)\n",
        "\n",
        "    # Tokenization using tweet tokenizer\n",
        "    tokens = tweet_tokenizer.tokenize(comment)\n",
        "\n",
        "    # Remove punctuation, lowercase, and keep only alphabetic words\n",
        "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Stemming\n",
        "    tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Remove stopwords from different languages\n",
        "    tokens = [token for token in tokens if token not in english_stopwords]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Spelling correction\n",
        "    corrected_tokens = []\n",
        "    for token in tokens:\n",
        "        blob = TextBlob(token)\n",
        "        corrected_token = blob.correct().raw\n",
        "        corrected_tokens.append(corrected_token)\n",
        "\n",
        "\n",
        "    # Join tokens back into a processed comment\n",
        "    processed_comment = ' '.join(tokens)\n",
        "    processed_comments.append(processed_comment)\n",
        "\n",
        "# Add processed comments to a new DataFrame\n",
        "processed_data = pd.DataFrame({'Processed_Comment': processed_comments})\n",
        "\n",
        "# Save the pre-processed data to a new CSV file\n",
        "preprocessed_csv_file = \"preprocessed_comments.csv\"\n",
        "processed_data.to_csv(preprocessed_csv_file, index=False)"
      ],
      "metadata": {
        "id": "6kmIC0I4izn8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}